Learning Markov Processes this article, we restrict our attention to discrete time dynamical systems.) Typically we do not know the exact dynamics of the system, so instead we consider a probabilistic state transition function: P (X t+1 jX t ). Such a probabilistic formulation will be particularly useful when we try to learn the model from data. The state space,  might be discrete (nite) or continuous (innite). For example, we might just try to predict the probability that a stock goes up or down, in which case  = f"; #g; more ambitiously, we might try to predict its expected value, in which case  = IR. In general, the state is a vector of state variables, which we can partition into three kinds: input variables (ones which we can control), output variables (ones which we can observe), and hidden or latent variables (internal variables which we cannot directly control or observe); we shall denote these by U t , Y t and X t  respectively. See Figure 1. In this article, we shall consider how to learn models of this kind. We start by considering the special case in    To be published in The Encyclopedia of Cognitive Science, Macmillan, 2002 1  X1 X2 X3 Y1 Y2 Y3 U1 U2  . . . Figure 1: A generic discrete-time dynamical system represented as a dynamic Bayesian network (DBN) (see BAYESIAN BELIEF NETWORKS for a denition). U t is the input, X t is the hidden state, and Y t is the output. Shaded nodes are observed, clear nodes are hidden. Square nodes are xed inputs (controls), round nodes are random variables. Notice how what we see, Y t , may depend on the actions that we take, U t : this can be used to model active perception. X1 X2 X3 X4  . . . X1 X2 X2 X3 X3 X4  Figure 2: Converting a second order Markov model (top) into a rst order Markov model (bottom). which all variables are observed (i.e....
