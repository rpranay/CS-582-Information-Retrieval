Using Labeled and Unlabeled Data to Learn Drifting Concepts For many learning tasks, where data is collected  over an extended period of time, one has to cope  two problems. The distribution underlying the data  is likely to change and only little labeled training  data is available at each point in time. A typical  example is information filtering, i. e. the adaptive  classification of documents with respect to a particular  user interest. Both the interest of the user  and the document content change over time. A filtering  system should be able to adapt to such concept  changes. Since users often give little feedback,  a filtering system should also be able to achieve a  good performance, even if only few labeled training  examples are provided. This paper proposes a  method to recognize and handle concept changes  with support vector machines and to use unlabeled  data to reduce the need for labeled data. The  method maintains windows on the training data,  whose size is automatically adjusted so that the estimated  generalization error is minimized. The approach  is both theoretically well-founded as well  as effective and efficient in practice. Since it does  not require complicated parameterization, it is simpler  to use and more robust than comparable heuristics.  Experiments with simulated concept drift scenarios  based on real-world text data compare the  new method with other window management approaches  and show that it can effectively select an  appropriate window size in a robust way. In order  to achieve an acceptable performance with fewer  labeled training examples, the proposed method exploits  unlabeled examples in a transductive way.  1 
