Event-Learning And Robust Policy Heuristics . In this paper we introduce a novel form of reinforcement learning called event-learning or E-learning. Events are ordered pairs of consecutive states. We define the corresponding event-value function. Learning rules which are guaranteed to converge to the optimal event-value function are derived. Combining our method with a known robust control method, the SDS algorithm, we introduce Robust Policy Heuristics (RPH). It is shown that RPH (a fast-adapting non-Markovian policy) is particularly useful for coarse models of the environment and for partially observed systems. Fast adaptation may allow to separate the time scale of learning to control a Markovian process and the time scale of adaptation of a non-Markovian policy. In our E-learning framework the de nition of modules is straightforward. E-learning is well suited for policy switching and planning, whereas RPH alleviates the `curse of dimensionality' problem. Computer simulations of a two-link pendulum with coarse discretization and noisy controller are shown to demonstrate the underlying principle. Date: First version: May 14, 2001, second version: May 19, 2001. Key words and phrases. reinforcement learning, robust control, event representation, continuous dynamical systems, non-Markovian policy. This work was supported by the Hungarian National Science Foundation (Grant OTKA 32487.) Thanks are due to Tor M. Aamodt for providing his double pendulum software [1] and to Gabor Szirtes for careful reading of the manuscript. THIS WORK IS A SHORTENED VERSION OF THE THESIS WORK OF I. POLIK AND I. SZITA SUPERVISED BY A. LORINCZ (SUBMITTED ON DECEMBER 6, 2000) THAT WON FIRST PRIZE IN THE STUDENT COMPETITION IN HUNGARY IN SECTION `MATHEMATICAL METHODS FOR COMPUTERS' ON APRIL 11, 2001
