Information-Theoretic Learning This chapter seeks to extend the ubiquitous mean-square error criterion (MSE) to cost functions that include more information about the training data. Since the learning process ultimately should transfer the information carried in the data samples onto the system's parameters, a natural goal is to find cost functions that directly manipulate information. Hence the name informationtheoretic learning (ITL). In order to be useful, ITL should be independent of the learning machine architecture, and require solely the availability of the data, i.e. it should not require a priori assumptions about the data distributions. The chapter presents our current efforts to develop ITL criteria based on the integration of nonparametric density estimators with Renyi's quadratic entropy definition. As a motivation we start with an application of the MSE to manipulate information using the nonlinear characteristics of the learning machine. This section illustrates the issues faced when we attempt to use...
