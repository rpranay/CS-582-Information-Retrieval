Using Models of Score Distributions in Information Retrieval Empirical modeling of a number of different text search engines shows that the score distributions on a per query basis may be fitted approximately using an exponential distribution for the set of nonrelevant documents and a normal distribution for the set of relevant documents. This model fits not only probabilistic search engines like INQUERY but also vector space search engines like SMART and also LSI search engines. The model also appears to be true of search engines operating on a number of different languages. This leads to the hypothesis that all 'good' text search engines operating on any language have similar characteristics. The question then arises as to whether the shape of the score distributions reflects some underlying model of language or the search process itself. We discuss how they arise given certain assumptions about word distributions in documents. We then show that given a query for which relevance information is not available, a mixture model consisting of an exponential and a normal distribution can be fitted to the score distribution. These distributions can be used to map the scores of a search engine to probabilities. This model has many possible applications. For example, the outputs of different search engines can be combined by averaging the probabilities (optimal if the search engines are independent) or by using the probabilities to select the best engine for each query. Results show that the technique performs as well as the best current combination techniques. A number of different IR tasks may benefit from score modeling including filtering, multi-lingual retrieval and relevance feedback. We also discuss possible future improvements to the process of score modeling. 1.
