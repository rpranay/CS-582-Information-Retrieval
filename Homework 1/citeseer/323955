Hierarchical Memory-Based Reinforcement Learning A key challenge for reinforcement learning is scaling up to large  partially observable domains. In this paper, we show how a hierarchy  of behaviors can be used to create and select among variable  length short-term memories appropriate for a task. At higher levels  in the hierarchy, the agent abstracts over lower-level details  and looks back over a variable number of high-level decisions in  time. We formalize this idea in a framework called Hierarchical  Sux Memory (HSM). HSM uses a memory-based SMDP learning  method to rapidly propagate delayed reward across long decision  sequences. We describe a detailed experimental study comparing  memory vs. hierarchy using the HSM framework on a realistic  corridor navigation task.  1 Introduction  Reinforcement learning encompasses a class of machine learning problems in which an agent learns from experience as it interacts with its environment. One fundamental challenge faced by reinforcement learning agents in real-world problems is that ...
